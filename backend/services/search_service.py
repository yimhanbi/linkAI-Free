
import time
import re
import json
from typing import List, Tuple, Optional

from llama_index.core import VectorStoreIndex, StorageContext, QueryBundle
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core.postprocessor.types import BaseNodePostprocessor

from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, MinShould
from llama_index.vector_stores.qdrant import QdrantVectorStore

import os

#-----------------------------------
#í™˜ê²½ ë³€ìˆ˜
QDRANT_URL = os.getenv("QDRANT_URL")
COLLECTION_NAME = os.getenv("PATENTS_COLLECTION_NAME")
OLLAMA_BASE_URL= os.getenv("OLLAMA_BASE_URL")
LLM_MODEL= os.getenv("OLLAMA_LLM_MODEL")
EMBED_MODEL= os.getenv("OLLAMA_EMBED_MODEL")
RETRIEVER_TOP_K = int(os.getenv("RETRIEVER_TOP_K", "30"))     # ë²¡í„° ê²€ìƒ‰ ìƒìœ„ Kê°œ
RERANKER_TOP_K = int(os.getenv("RERANKER_TOP_K", "6"))       # Reranking í›„ ìƒìœ„ Kê°œ
METADATA_TOP_K = int(os.getenv("METADATA_TOP_K", "10"))      # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ìƒìš° Kê°œ


#-------------------------------
#ì „ì—­ ë³€ìˆ˜
client:Optional[QdrantClient] = None
index: Optional[VectorStoreIndex]= None
retriever= None
reranker= None
synth= None 


#--------------------------------
#ë¶ˆìš©ì–´ ë° ì¡°ì‚¬ ì •ì˜
#--------------------------------

#ê²€ìƒ‰ ì‹œ ì œì™¸í•  ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë“¤
STOPWORDS = {
    "í•˜ë‹¤","í•´ì¤˜","ì•Œë ¤ì¤˜","ì°¾ì•„ì¤˜","ê´€ë ¨",
    "íŠ¹í—ˆ","ë°œëª…","ë°œëª…í•œ"
}

# í•œêµ­ì–´ ì¡°ì‚¬ ëª©ë¡ 
JOSA_SUFFIX = (
    "ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼",
    "ì˜","ì—","ì—ì„œ","ìœ¼ë¡œ","ì™€","ê³¼","ë„","ë§Œ"
)


#--------------------------------

class MetaPrefixPostprocessor(BaseNodePostprocessor):
    
    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_str: Optional[str] = None,
    ) -> List[NodeWithScore]:
        
        for nws in nodes:
            node = nws.node
            meta = node.metadata or {}
            
            
        #ë©”íƒ€ë°ì´í„°ì—ì„œ íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ
        patent_no = meta.get("patent_no","")
        application_no = meta.get("application_number","")
        title = meta.get("title","")
        
        #ë©”íƒ€ë°ì´í„°ì—ì„œ í”„ë¦¬í”½ìŠ¤ ìƒì„±
        prefix= (
            f"[META]\n"
            f" - ê³µê°œë²ˆí˜¸: {patent_no}\n"
            f" - ì¶œì›ë²ˆí˜¸: {application_no}\n"
            f" - ì œëª©:  {title}\n"
            f"[/META]\n\n"
        )
        
        #ê¸°ì¡´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        orig = getattr(node,"text",None)
        if not orig:
            orig = node.get_content() or ""
            
            
        #ì´ë¯¸ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if orig.startswith("[META]\n"):
            node.text= orig
        else:
            node.text = prefix + orig
            
        return nodes 


#--------------------------------------
# í† í°í™” í•¨ìˆ˜
#--------------------------------------
def simple_tokenize_korean(query: str) -> List[str]:
    # 1. í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
    raw_tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", query)
    cleaned = []
    
    for tok in raw_tokens:
        # 2. ì¡°ì‚¬ ì œê±° (ëì—ì„œë¶€í„° ë§¤ì¹­)
        for josa in JOSA_SUFFIX:
            if tok.endswith(josa) and len(tok) > len(josa):
                tok = tok[:-len(josa)]  # ì¡°ì‚¬ ë¶€ë¶„ ì œê±°
                break
        
        # 3. ë¶ˆìš©ì–´ ë° ì§§ì€ ë‹¨ì–´ ì œê±°
        if tok in STOPWORDS or len(tok) <= 1:
            continue
        
        cleaned.append(tok)
    
    return cleaned


#--------------------------------------
# ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
#--------------------------------------
def qdrant_meta_search(
    tokens: List[str],
    limit: int,
) -> List[NodeWithScore]:
    

    if not tokens:
        return []

    # ê° í† í°ì— ëŒ€í•´ ì—¬ëŸ¬ í•„ë“œì—ì„œ ê²€ìƒ‰ ì¡°ê±´ ìƒì„±
    should_conditions = []
    for t in tokens:
        should_conditions.extend([
            FieldCondition(key="applicants", match=MatchValue(value=t)),        # ì¶œì›ì¸
            FieldCondition(key="inventors", match=MatchValue(value=t)),         # ë°œëª…ì
            FieldCondition(key="agents", match=MatchValue(value=t)),           # ëŒ€ë¦¬ì¸
            FieldCondition(key="patent_no", match=MatchValue(value=t)),        # ê³µê°œë²ˆí˜¸
            FieldCondition(key="application_number", match=MatchValue(value=t)), # ì¶œì›ë²ˆí˜¸
        ])

    # í•„í„° ìƒì„±: sectionì´ "doc_meta"ì´ê³ , ìœ„ ì¡°ê±´ ì¤‘ ìµœì†Œ 1ê°œ ë§Œì¡±
    flt = Filter(
        must=[FieldCondition(key="section", match=MatchValue(value="doc_meta"))],
        min_should=MinShould(conditions=should_conditions, min_count=1),
    )

    # Qdrant ê²€ìƒ‰ ì‹¤í–‰
    hits, _ = client.scroll(
        collection_name=COLLECTION_NAME,
        scroll_filter=flt,
        limit=limit,
        with_payload=True,   # ë©”íƒ€ë°ì´í„° í¬í•¨
        with_vectors=False,  # ë²¡í„°ëŠ” ë¶ˆí•„ìš”
    )

    # ê²°ê³¼ë¥¼ NodeWithScore í˜•íƒœë¡œ ë³€í™˜
    out = []
    for h in hits:
        payload = h.payload or {}
        raw = payload.get("_node_content") or payload.get("text") or ""

        # JSON í˜•íƒœë©´ íŒŒì‹±
        text = raw
        if isinstance(raw, str) and raw.startswith("{"):
            try:
                obj = json.loads(raw)
                text = obj.get("text") or obj.get("_node_content") or raw
            except Exception:
                text = raw

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (text, _node_content ì œì™¸)
        meta = {k: v for k, v in payload.items() if k not in ("text", "_node_content")}
        
        # TextNode ìƒì„±
        node = TextNode(text=text, metadata=meta)
        out.append(NodeWithScore(node=node, score=0.0))
    
    return out


#--------------------------------------
# ì¶œì²˜ ì¶”ì¶œ
#--------------------------------------
def print_sources(nodes: List[NodeWithScore]) -> List[dict]:
    """
     combined_nodesì—ì„œ ì¶œì²˜(ê³µê°œë²ˆí˜¸, ì¶œì›ë²ˆí˜¸, ì œëª©)ë¥¼
    ì¤‘ë³µ ì œê±°í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
    """
    
    seen = set()  # ì¤‘ë³µ ì²´í¬ìš©
    results = []

    for nws in nodes:
        meta = nws.node.metadata or {}
        patent_no = meta.get("patent_no")
        application_no = meta.get("application_number")
        title = meta.get("title")

        # ì •ë³´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if not (patent_no or application_no or title):
            continue

        # ì¤‘ë³µ ì²´í¬ (ì„¸ í•„ë“œ ì¡°í•©ìœ¼ë¡œ íŒë‹¨)
        key = (patent_no, application_no, title)
        if key in seen:
            continue

        seen.add(key)
        results.append(key)
        
        
    if not results:
        print("\n===ì¶œì²˜ ===\nì—†ìŒ")
    return results


    print("\n=== ì¶œì²˜ ===\n")
    for i, (pno, ano, title) in enumerate(results, 1):
        print(f"[{i}] ê³µê°œë²ˆí˜¸: {pno}")
        print(f"    ì¶œì›ë²ˆí˜¸: {ano}")
        print(f"    íŠ¹í—ˆì œëª©: {title}")
        print()





#--------------------------------------
# ì´ˆê¸°í™” í•¨ìˆ˜
#--------------------------------------
async def initialize_llamaindex():
    global client, index, retriever, reranker, synth
    
    start = time.time()
    print("â–¶ Initializing LlamaIndex with Ollama ({LLM_MODEL})...")
    
    
    try:
        #Ollama ëª¨ë¸ LlamaIndex ì „ì—­ ì„¤ì •ì— ì£¼ì… --
        #1. LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
        Settings.llm = Ollama(
            model = LLM_MODEL,
            base_url = OLLAMA_BASE_URL,
            request_timeout = 300.0 #ì„œë²„ íƒ€ì„ì•„ì›ƒ 5ë¶„ 
        )
        
        #2. Embedding ì„¤ì • 
        Settings.embed_model = OllamaEmbedding(
            model_name = EMBED_MODEL,
            base_url = OLLAMA_BASE_URL
        )
        
        # --------------------------------------------------
        
        # 1. Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = QdrantClient(url=QDRANT_URL)
        print("â–¶ Qdrant Connected")
        
        # 2. Vector Store & Index ìƒì„±
        vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            storage_context=storage_context,
        )
        
        # 3. Retriever ìƒì„± (ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰)
        retriever = index.as_retriever(similarity_top_k=RETRIEVER_TOP_K)
        
        # 4. Reranker ìƒì„± (Cross-Encoderë¡œ ê²°ê³¼ ì¬ì •ë ¬)
        # ë” ì •í™•í•œ ê´€ë ¨ì„± íŒë‹¨ì„ ìœ„í•´ ì‚¬ìš©
        reranker = SentenceTransformerRerank(
            model="cross-encoder/ms-marco-MiniLM-L-6-v2",
            top_n=RERANKER_TOP_K,
        )
        
        # 5. Response Synthesizer ìƒì„± (LLM ë‹µë³€ ìƒì„±)
        synth = get_response_synthesizer()
        
        elapsed = time.time() - start
        print(f"âœ… LlamaIndex engine initialized in {elapsed:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"ì´ˆê¸°í™” ì‹¤íŒ¨:{e}")
        import traceback
        traceback.print_exc()
        raise e
    
    
        



#--------------------------------------
# ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
#--------------------------------------
async def run_llamaindex_query(query: str,top_k: int = 30) -> Tuple[str, List[dict]]:
    overall_start = time.time()
    
    print(f"\n{'#'*70}")
    print(f"ğŸ” [LlamaIndex RAG ì‹œì‘]")
    print(f"   Query: '{query}'")
    print(f"{'#'*70}")
    
    # 1. ì¿¼ë¦¬ ë²ˆë“¤ ìƒì„± (LlamaIndex ë‚´ë¶€ í˜•ì‹)
    qb = QueryBundle(query)
    
    # 2. ë²¡í„° ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)
    retrieve_start = time.time()
    base_nodes = retriever.retrieve(qb)
    retrieve_elapsed = time.time() - retrieve_start
    print(f"â±ï¸  [1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰] {retrieve_elapsed:.2f}ì´ˆ â†’ {len(base_nodes)}ê°œ ë…¸ë“œ")
    
    # 3. Reranking (ë” ì •í™•í•œ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì¬ì •ë ¬)
    rerank_start = time.time()
    reranked_nodes = reranker.postprocess_nodes(base_nodes, query_bundle=qb)
    rerank_elapsed = time.time() - rerank_start
    print(f"â±ï¸  [2ë‹¨ê³„: Reranking] {rerank_elapsed:.2f}ì´ˆ â†’ {len(reranked_nodes)}ê°œ ë…¸ë“œ")
    
    # 4. ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (ì¶œì›ì¸, ë°œëª…ì ë“± ì§ì ‘ ë§¤ì¹­)
    meta_start = time.time()
    
    # 4-1. ì¿¼ë¦¬ í† í°í™” (ì¡°ì‚¬ ì œê±°, ë¶ˆìš©ì–´ ì œê±°)
    tokens = simple_tokenize_korean(query)
    print(f"ğŸ” [í† í°í™”] {tokens}")
    
    # 4-2. ë©”íƒ€ë°ì´í„° í•„ë“œì—ì„œ í† í° ê²€ìƒ‰
    meta_nodes = qdrant_meta_search(
        tokens=tokens,
        limit=METADATA_TOP_K,
    )
    meta_elapsed = time.time() - meta_start
    print(f"â±ï¸  [3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰] {meta_elapsed:.2f}ì´ˆ â†’ {len(meta_nodes)}ê°œ ë…¸ë“œ")
    
    # 5. ë…¸ë“œ ê²°í•© ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
    combine_start = time.time()
    
    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ + ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ê²°ê³¼ í•©ì¹˜ê¸°
    combined_nodes = list(reranked_nodes) + list(meta_nodes)
    
    # ê° ë…¸ë“œì— [META] íƒœê·¸ ì¶”ê°€
    meta_prefix = MetaPrefixPostprocessor()
    combined_nodes = meta_prefix.postprocess_nodes(combined_nodes, query_bundle=qb)
    
    combine_elapsed = time.time() - combine_start
    print(f"â±ï¸  [4ë‹¨ê³„: ë…¸ë“œ ê²°í•©] {combine_elapsed:.2f}ì´ˆ â†’ {len(combined_nodes)}ê°œ ë…¸ë“œ")
    
    # 6. LLM ë‹µë³€ ìƒì„±
    llm_start = time.time()
    print(f"\nğŸ§  [5ë‹¨ê³„: LLM ë‹µë³€ ìƒì„± ì¤‘...]")
    
    # LLMì—ê²Œ contextì™€ queryë¥¼ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
    resp = synth.synthesize(qb, combined_nodes)
    answer = str(resp)
    
    llm_elapsed = time.time() - llm_start
    print(f"â±ï¸  [5ë‹¨ê³„: LLM ë‹µë³€] {llm_elapsed:.2f}ì´ˆ")
    print(f"   - ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
    
    # 7. ì¶œì²˜ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
    sources = print_sources(combined_nodes)
    
    # ì „ì²´ ì‹œê°„ ê³„ì‚° ë° ë¡œê·¸ ì¶œë ¥
    overall_elapsed = time.time() - overall_start
    
    print(f"\n{'='*70}")
    print(f"âœ… [ì „ì²´ ì™„ë£Œ] {overall_elapsed:.2f}ì´ˆ")
    print(f"{'='*70}")
    print(f"ğŸ“Š [ì‹œê°„ ë¶„í•´]")
    print(f"   1. ë²¡í„° ê²€ìƒ‰:      {retrieve_elapsed:6.2f}ì´ˆ ({retrieve_elapsed/overall_elapsed*100:5.1f}%)")
    print(f"   2. Reranking:      {rerank_elapsed:6.2f}ì´ˆ ({rerank_elapsed/overall_elapsed*100:5.1f}%)")
    print(f"   3. ë©”íƒ€ ê²€ìƒ‰:      {meta_elapsed:6.2f}ì´ˆ ({meta_elapsed/overall_elapsed*100:5.1f}%)")
    print(f"   4. ë…¸ë“œ ê²°í•©:      {combine_elapsed:6.2f}ì´ˆ ({combine_elapsed/overall_elapsed*100:5.1f}%)")
    print(f"   5. LLM ë‹µë³€:       {llm_elapsed:6.2f}ì´ˆ ({llm_elapsed/overall_elapsed*100:5.1f}%)")
    print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"   ì´ ì‹œê°„:          {overall_elapsed:6.2f}ì´ˆ")
    print(f"{'='*70}\n")
    
    return answer, sources
